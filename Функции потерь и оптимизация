#Загрузите данные. Используйте датасет с ирисами. Его можно загрузить непосредственно из библиотеки Sklearn. В данных оставьте только 2 класса: Iris Versicolor, Iris Virginica.

import numpy as np
import time
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder

iris = load_iris()
data1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']],
                     columns= iris['feature_names'] + ['target'])
data2 = data1.loc[data1['target'] != 0.0]
data2.head()

data2.info()

#Самостоятельно реализуйте логистическую регрессию, без использования метода LogisticRegression из библиотеки. Можете использовать библиотеки pandas, numpy, math для реализации. Оформите в виде функции. Оформите в виде класса с методами.

class LogisticRegression:
  def __init__(self,epoch=1000,learning_rate=0.01,learn_method='sgd',stop_rate=0.0001): 
    self.epoch = epoch 
    self.learn_method = learn_method 
    self.learning_rate = learning_rate 
    self.stop_rate = stop_rate 
    self.learn_func = None 
    self.cost_func = None

    self.weights = None
    self.epoch_passed = 0
    self.learn_time = 0

    if(self.learn_method == 'sgd'):
      self.learn_func = self.learn_sgd
    elif(self.learn_method == 'rmsprop'):
      self.learn_func = self.learn_rmsprop
    elif(self.learn_method == 'adam'):
      self.learn_func = self.learn_adam
    elif(self.learn_method == 'nadam'):
      self.learn_func = self.learn_nadam
    else:
      raise Exception('Unknown learining method: {}'.format(self.learn_method))

  def init_weights(self,X):
    #return np.random.randn(X.shape[1], 1);
    return np.zeros((X.shape[1], 1))

  def fit(self,X,Y):
    X = self.addones(X)
    Y = np.reshape(Y, (len(Y), 1))
    W = self.init_weights(X)

    self.epoch_passed = 0
    self.learn_time = 0

    start = time.perf_counter()
    self.learn_func(X,Y,W)
    self.learn_time = time.perf_counter() - start


  def score(self,X,Y):
    predictions = self.predict(X)
    scores = []
    for idx, y_pred in enumerate(predictions):
      # заполняем True если предсказание верное, иначе False
      scores.append(True if y_pred == Y[idx] else False)

    return scores.count(True)/len(scores)

  def n_iter(self):
    return self.epoch_passed

  def time(self):
    return self.learn_time    

  def predict_proba(self,X):
    X = self.addones(X)

    return self.sigmoid_linear_regression(X,self.weights)

  def predict(self,X):
    probas = self.predict_proba(X)
    for idx, y in enumerate(probas):
      probas[idx] = 1 if y > 0.5 else 0

    return probas

  def addones(self,X):
    return np.hstack((np.ones((X.shape[0],1)),X))

  def learn_sgd(self,X,Y,W):
    prev = self.cost_binary_cross_entropy(X,Y,W)
    for enum in range(self.epoch):
      self.epoch_passed = enum+1
      gradients = self.gradient(X,Y,W)
      W = W-self.learning_rate*gradients
      actual = self.cost_binary_cross_entropy(X,Y,W)
      diff = abs(prev - actual)
      if(diff <= self.stop_rate):
        break
      prev = actual
      
    self.weights = W
    return W

  def learn_rmsprop(self,X,Y,W):
    prev = self.cost_binary_cross_entropy(X,Y,W)
    lr = self.learning_rate
    cached_rmsprop = [0] * len(W)
    decay_rate = 0.9
    for enum in range(self.epoch):
      self.epoch_passed = enum+1

      gradients = self.gradient(X,Y,W)
      NEW_W = []
      for i, (w, grad) in enumerate(zip(W, gradients)):
        cached_rmsprop[i] = decay_rate * cached_rmsprop[i] + (1-decay_rate) * grad **2
        new_w = w-lr*grad/(np.sqrt(cached_rmsprop[i]+1e-6))
        NEW_W.append(new_w)

      W = NEW_W
      actual = self.cost_binary_cross_entropy(X,Y,W)
      diff = abs(prev - actual)
      if(diff <= self.stop_rate):
        break
      prev = actual
      
    self.weights = W
    return W

  def learn_adam(self,X,Y,W):
    prev = self.cost_binary_cross_entropy(X,Y,W)
    lr = self.learning_rate
    m = [0] * len(W)
    v = [0] * len(W)
    t = 1
    beta1 = 0.9
    beta2 = 0.999
    for enum in range(self.epoch):
      self.epoch_passed = enum+1

      gradients = self.gradient(X,Y,W)
      NEW_W = []
      for i, (w, grad) in enumerate(zip(W, gradients)):
        m[i] = beta1 * m[i] + (1-beta1) * grad
        v[i] = beta2 * v[i] + (1-beta2) * grad **2
        m_corrected = m[i]/(1-beta1**t)
        v_corrected = v[i]/(1-beta2**t)

        new_w = w-lr*m_corrected/(np.sqrt(v_corrected+1e-8))

        NEW_W.append(new_w)

      t += 1

      W = NEW_W
      actual = self.cost_binary_cross_entropy(X,Y,W)
      diff = abs(prev - actual)
      if(diff <= self.stop_rate):
        break
      prev = actual
      
    self.weights = W
    return W

  def learn_nadam(self,X,Y,W):
    prev = self.cost_binary_cross_entropy(X,Y,W)
    lr = self.learning_rate
    m = [0] * len(W)
    v = [0] * len(W)
    t = 1
    beta1 = 0.9
    beta2 = 0.999
    for enum in range(self.epoch):
      self.epoch_passed = enum+1
      
      gradients = self.gradient(X,Y,W)
      NEW_W = []
      for i, (w, grad) in enumerate(zip(W, gradients)):
        m[i] = beta1 * m[i] + (1-beta1) * grad
        v[i] = beta2 * v[i] + (1-beta2) * grad **2
        m_corrected = m[i]/(1-beta1**t)
        v_corrected = v[i]/(1-beta2**t)

        new_w = w - (lr/(np.sqrt(v_corrected)+1e-8)*(beta1*m_corrected+(1-beta1)*grad/(1-beta1**t)))

        NEW_W.append(new_w)

      t += 1

      W = NEW_W
      actual = self.cost_binary_cross_entropy(X,Y,W)
      diff = abs(prev - actual)
      if(diff <= self.stop_rate):
        break
      prev = actual
      
    self.weights = W
    return W

  def cost_binary_cross_entropy(self,X,Y,W):
    m = X.shape[0]
    total_cost = -(1 / m) * np.sum(
        Y * np.log(self.sigmoid_linear_regression(X, W)) + (1 - Y) * np.log(
            1 - self.sigmoid_linear_regression(X, W)))
    return total_cost    

  def gradient(self, X,Y,W):
    m = X.shape[0]
    return (1 / m) * np.dot(X.T, self.sigmoid_linear_regression(X,W) - Y)    

  def sigmoid_linear_regression(self,X,W):
    return self.sigmoid(self.linear_regression(X,W))

  def linear_regression(self,X,W):
    return np.dot(X,W)

  def sigmoid(self,H):
    return 1/(1+np.exp(-H))
    
 Реализуйте метод градиентного спуска. Обучите логистическую регрессию этим методом. Выберете и посчитайте метрику качества. Метрика должна быть одинакова для всех пунктов домашнего задания. Для упрощения сравнения выберете только одну метрику.
 
 data2.columns
 
 X = data2[['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']]
result_y = 'target'
le = LabelEncoder()
le.fit(data2[result_y])
y = le.transform(data2[result_y])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model_1 = LogisticRegression(epoch=1000,learn_method='sgd')
model_1.fit(X_train, y_train)
score_1= model_1.score(X_test,y_test)
score_1

#Повторите п. 3 для метода скользящего среднего (Root Mean Square Propagation, RMSProp)
model_2 = LogisticRegression(learn_method='rmsprop',epoch=1000)
model_2.fit(X_train, y_train)
score_2 = model_2.score(X_test,y_test)
score_2

#Повторите п. 3 для ускоренного по Нестерову метода адаптивной оценки моментов (Nesterov–accelerated Adaptive Moment Estimation, Nadam).

model_3 = LogisticRegression(learn_method='adam',epoch=1000)
model_3.fit(X_train, y_train)
score_3 = model_3.score(X_test,y_test)
score_3

model_4 = LogisticRegression(learn_method='nadam',epoch=1000)
model_4.fit(X_train, y_train)
score_4 = model_4.score(X_test,y_test)
score_4

#Сравните значение метрик для реализованных методов оптимизации. Можно оформить в виде таблицы вида |метод|метрика|время работы| (время работы опционально). Напишите вывод.

results = pd.DataFrame(columns=['Method','Score','Iterations','Time'])
iter_1 = model_1.n_iter()
time_1 = model_1.time()
results.loc[len(results)] = ['SGD',score_1,iter_1,time_1]

iter_2 = model_2.n_iter()
time_2 = model_2.time()
results.loc[len(results)] = ['Rmsprop',score_2,iter_2,time_2]

iter_3 = model_3.n_iter()
time_3 = model_3.time()
results.loc[len(results)] = ['Adam',score_3,iter_1,time_3]

iter_4 = model_4.n_iter()
time_4 = model_4.time()
results.loc[len(results)] = ['Nadam',score_4,iter_4,time_4]
results


