##1.Получите данные и загрузите их в рабочую среду.

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import preprocessing
import sklearn.metrics

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.ensemble import IsolationForest

from sklearn import datasets, linear_model
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.svm import LinearSVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import StackingClassifier

data=pd.read_csv('heart.csv')
data.head()

##2.Подготовьте датасет к обучению моделей.
#Категориальные переменные переведите в цифровые значения. Можно использовать pd.get_dummies, preprocessing.LabelEncoder. Старайтесь не использовать для этой задачи циклы.

data.columns

selectedColumns = data[['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',
       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope','HeartDisease']]
df_updated = pd.get_dummies(selectedColumns)
df_updated.head()

##*Постройте 1-2 графика на выбор. Визуализация должна быть основана на исследуемых данных и быть полезной (из графика можно сделать вывод об особенностях датасета/класса/признака).

plt.figure(figsize=(14,12), dpi= 80)
sns.heatmap(df_updated.corr(), xticklabels=df_updated.corr().columns, yticklabels=df_updated.corr().columns, cmap='RdYlGn', center=0, annot=True)

plt.title('Data correlation', fontsize=22)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.show()

##3.Разделите выборку на обучающее и тестовое подмножество. 80% данных оставить на обучающее множество, 20% на тестовое.

X = df_updated.drop(['HeartDisease'],axis=1)
y = df_updated['HeartDisease']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

##4.Обучите дерево решений на обучающем множестве. Используйте следующие модели:
#tree.DecisionTreeClassifier

classifier = DecisionTreeClassifier()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

clf = RandomForestClassifier(max_depth=3, random_state=0)
clf.fit(X_train, y_train)
y_pred1 = clf.predict(X_test)

#ensemble.RandomForestClassifier

##5.Для тестового множества сделайте предсказание целевой переменной. Выведите метрики для каждой построенной модели с помощью metrics.classification_report.

#For DecisionTreeClassifier()
print(classification_report(y_test, y_pred, target_names=['0','1']))

#For RandomForestClassifier()
print(classification_report(y_test, y_pred1, target_names=['0','1']))

##6.Выведите важность признаков, полученную после обучения модели из п. 4b в виде столбчатой диаграммы. Отсортируйте важность по убыванию.

importance = sorted(clf.feature_importances_,reverse = True)

plt.figure(figsize=(10,6), dpi= 80)
plt.title('Feature Importance', fontsize=22)
plt.bar(([x for x in range(len(importance))]),importance)
plt.xticks([x for x in range(len(importance))],rotation = 270,fontsize=10)
plt.yticks(fontsize=10)
plt.show()

##7.Обучите бэггинг над моделью из п. 4a. Используйте ensemble.BaggingClassifier.
#Повторите п. 5

Bclf = BaggingClassifier(base_estimator=SVC(),n_estimators=10, random_state=0).fit(X_train, y_train)
y_pred2 = Bclf.predict(X_test)

#For BaggingClassifier
print(classification_report(y_test, y_pred2, target_names=['0','1']))

##8.Обучите стекинг трех моделей: из п. 4a, п. 4b и svm.LinearSVC. Используйте ensemble.StackingClassifier.
#Повторите п. 5

Sclf = make_pipeline(StandardScaler(),LinearSVC(random_state=42))
Sclf.fit(X_train, y_train)

estimators = [
     ('DTC',DecisionTreeClassifier()),
     ('RF', RandomForestClassifier(max_depth=3, random_state=0)),
     ('SVR', make_pipeline(StandardScaler(),LinearSVC(random_state=42)))
             ]
SCclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
SCclf.fit(X_train, y_train)

y_pred_SCclf = SCclf.predict(X_test)
#For StackingClassifier
print(classification_report(y_test, y_pred_SCclf, target_names=['0','1']))





